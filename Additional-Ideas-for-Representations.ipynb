{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use the WordNet (sloWnet for Slovene) and merge all synonyms (based on https://stackoverflow.com/questions/63705803/merge-related-words-in-nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from PyDictionary import PyDictionary\n",
    "\n",
    "def normalize_textual_information(text):\n",
    "   # split text into tokens by white space\n",
    "   token = text.split()\n",
    "\n",
    "   # remove punctuation from each token\n",
    "   table = str.maketrans('', '', punctuation)\n",
    "   token = [word.translate(table) for word in token]\n",
    "\n",
    "   # remove any tokens that are not alphabetic\n",
    "   token = [word.lower() for word in token if word.isalpha()]\n",
    "\n",
    "   # filter out English stop words\n",
    "   stop_words = set(stopwords.words('english'))\n",
    "\n",
    "   # you could add additional stops like this\n",
    "   stop_words.add('cannot')\n",
    "   stop_words.add('could')\n",
    "   stop_words.add('would')\n",
    "\n",
    "   token = [word for word in token if word not in stop_words]\n",
    "\n",
    "   # filter out any short tokens\n",
    "   token = [word for word in token if len(word) > 1]\n",
    "   return token\n",
    "\n",
    "\n",
    "def generate_word_frequencies(words):\n",
    "   # list to hold word frequencies\n",
    "   word_frequencies = []\n",
    "\n",
    "   # loop through the tokens and generate a word count for each token\n",
    "   for word in words:\n",
    "      word_frequencies.append(words.count(word))\n",
    "\n",
    "   # aggregates the words and word_frequencies into tuples and coverts them into a dictionary\n",
    "   word_frequencies = (dict(zip(words, word_frequencies)))\n",
    "\n",
    "   # sort the frequency of the words from low to high\n",
    "   sorted_frequencies = {key: value for key, value in \n",
    "   sorted(word_frequencies.items(), key=lambda item: item[1])}\n",
    "\n",
    " return sorted_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = normalize_textual_information(input_text)\n",
    "\n",
    "all_synsets_1 = {}\n",
    "for word in words:\n",
    "  for synonym in wordnet.synsets(word):\n",
    "    if word != synonym.name() and len(synonym.lemma_names()) > 1:\n",
    "      for item in synonym.lemmas():\n",
    "        if word != item.name():\n",
    "          all_synsets_1.setdefault(word, []).append(str(item.name()).lower())\n",
    "\n",
    "all_synsets_2 = {}\n",
    "for word in words:\n",
    "  word_synonyms = get_synonyms_internet(word)\n",
    "  for synonym in word_synonyms:\n",
    "    if word != synonym and synonym is not None:\n",
    "      all_synsets_2.update(synonym)\n",
    "\n",
    " word_relationship = {**all_synsets_1, **all_synsets_2}\n",
    "\n",
    " frequencies = generate_word_frequencies(words)\n",
    " word_matches = []\n",
    " word_set = {}\n",
    " duplication_check = set()\n",
    "\n",
    " for word, frequency in frequencies.items():\n",
    "    for keyword, synonym in word_relationship.items():\n",
    "       match = [x for x in synonym if word == x]\n",
    "       if word == keyword or match:\n",
    "         match = ' '.join(map(str, match))\n",
    "         if match not in word_set or match not in duplication_check or word not in duplication_check:\n",
    "            duplication_check.add(word)\n",
    "            duplication_check.add(match)\n",
    "            word_matches.append([keyword, match, frequency])\n",
    "\n",
    " # used to hold the final keyword and frequencies\n",
    " final_results = {}\n",
    "\n",
    " # list comprehension to obtain the primary keyword and its frequencies\n",
    " synonym_matches = [(keyword[0], keyword[2]) for keyword in word_matches]\n",
    "\n",
    " # iterate synonym_matches and output total frequency count for a specific keyword\n",
    " for item in synonym_matches:\n",
    "    if item[0] not in final_results.keys():\n",
    "      frequency_count = 0\n",
    "      frequency_count = frequency_count + item[1]\n",
    "      final_results[item[0]] = frequency_count\n",
    " else:\n",
    "    frequency_count = frequency_count + item[1]\n",
    "    final_results[item[0]] = frequency_count\n",
    "\n",
    "# do something with the final results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in range(len(representation_list[0])):\n",
    "   # appending an empty sub_list\n",
    "   representation_sets.append([])\n",
    "   # iterating over the list length\n",
    "   for representation in range(len(representation_list)):\n",
    "      # adding the element to the result\n",
    "      representation_sets[index].append(representation_list[representation][index])\n",
    "\n",
    "    # moving to the next index\n",
    "    index += 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d9cfed5f1ae94e54fa81532beaff0e3c2671211fce378bf56ff5a3766143a25d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
