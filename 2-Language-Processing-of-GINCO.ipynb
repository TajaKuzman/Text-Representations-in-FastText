{"cells":[{"cell_type":"markdown","metadata":{"id":"lQtB43PuAD2H"},"source":["# Preparing the dataset"]},{"cell_type":"markdown","metadata":{},"source":["Importing the necessary libraries"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T11:42:27.066949Z","iopub.status.busy":"2022-02-03T11:42:27.066458Z","iopub.status.idle":"2022-02-03T11:42:27.072531Z","shell.execute_reply":"2022-02-03T11:42:27.071727Z","shell.execute_reply.started":"2022-02-03T11:42:27.066908Z"},"trusted":true},"outputs":[],"source":["import json\n","import pandas as pd\n","from copy import deepcopy\n","import re\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T11:42:27.074930Z","iopub.status.busy":"2022-02-03T11:42:27.074539Z","iopub.status.idle":"2022-02-03T11:42:27.334910Z","shell.execute_reply":"2022-02-03T11:42:27.333513Z","shell.execute_reply.started":"2022-02-03T11:42:27.074889Z"},"executionInfo":{"elapsed":389,"status":"ok","timestamp":1640949108560,"user":{"displayName":"Taja Kuzman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16199926972677634189"},"user_tz":-60},"id":"qL9iYqif_lSk","trusted":true},"outputs":[],"source":["# Import the file\n","\n","with open(\"/kaggle/input/genre-identification-corpus-ginco-10/GINCO-1.0-suitable.json\") as f:\n","    dataset = json.load(f)\n","\n","dataset[0]"]},{"cell_type":"markdown","metadata":{"id":"qn5ISqRR3hpq"},"source":[" ## Extract text from paragraphs into one string: baseline text\n","\n"," We'll create an additional parameter for each document: \"keep_text\" with text non-duplicates and duplicates, marked as \"keep\" (useful for genre identification, although they are duplicates). After joining text, we'll delete the separate paragraphs to make the file smaller."]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T11:42:27.336991Z","iopub.status.busy":"2022-02-03T11:42:27.336539Z","iopub.status.idle":"2022-02-03T11:42:27.410617Z","shell.execute_reply":"2022-02-03T11:42:27.409445Z","shell.execute_reply.started":"2022-02-03T11:42:27.336954Z"},"trusted":true},"outputs":[],"source":["for instance in dataset:\n","    paragraphs = instance[\"paragraphs\"]\n","    # Removing duplicates:\n","    paragraphs = [p for p in paragraphs if p[\"keep\"]]\n","\n","    # Joining texts:\n","    instance_keep_text = \" \".join([p[\"text\"] for p in paragraphs])\n","    \n","    # Remove all symbols that raise an error in CLASSLA:\n","    gt_re = re.compile(\"&gt;\")\n","    lt_re = re.compile(\"&lt;\")\n","    instance_keep_text = gt_re.sub('',instance_keep_text)\n","    instance_keep_text = lt_re.sub('',instance_keep_text)\n","    \n","    # Assigning texts to a new field:\n","    instance[\"baseline_text\"] = instance_keep_text\n","    \n","    # Delete the text in paragraphs to save space:\n","    instance[\"paragraphs\"] = []\n","    \n","    # Add a new field for text length:\n","    list_baseline_text = []\n","    list_baseline_text = instance[\"baseline_text\"].split()    \n","    instance[\"no_of_words\"] = len(list_baseline_text)\n","    \n","dataset[0]"]},{"cell_type":"markdown","metadata":{},"source":["## Add text representations"]},{"cell_type":"markdown","metadata":{},"source":["Import the CLASSLA Pipeline for processing of Slovene"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T11:42:27.413378Z","iopub.status.busy":"2022-02-03T11:42:27.413060Z","iopub.status.idle":"2022-02-03T11:42:38.700004Z","shell.execute_reply":"2022-02-03T11:42:38.698724Z","shell.execute_reply.started":"2022-02-03T11:42:27.413348Z"},"trusted":true},"outputs":[],"source":["!pip install classla"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T11:42:38.702174Z","iopub.status.busy":"2022-02-03T11:42:38.701878Z"},"trusted":true},"outputs":[],"source":["# Import CLASSLA for language processing\n","import classla\n","classla.download(\"sl\")\n","nlp = classla.Pipeline('sl')"]},{"cell_type":"markdown","metadata":{},"source":["Let's first see how the pipeline works by processing the first text:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Run the processing on the first text\n","doc = nlp(dataset[0][\"baseline_text\"])\n","\n","dictionary = doc.to_dict()\n","dictionary[0][0][:2]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def process_dataset_json(dataset,file_name):\n","    \"\"\"\n","    This function performs linguistic processing of the text in the text[\"baseline_text\"] value\n","    by using the CLASSLA pipeline for processing of South Slavic languages.\n","    It adds keys lemmas, upos, xpos, ner and dependency to each dictionary element\n","    in the dataset containing information on each document.\n","    Finally, it adds the update content for each text dictionary\n","    to a JSON file which name is specified with the argument file_name.\n","    \n","    Be aware that if a text contains more than 3000 words, you might need to split the text\n","    into multiple instances to avoid overloading the memory.\n","\n","    Args:\n","        dataset (str): name of the dataset object\n","        file_name(str): name of the JSON file where the new dataset is written\n","    \"\"\"\n","    json_dict = []\n","        \n","    for text in tqdm(dataset):\n","            if len(text[\"baseline_text\"]) > 0:\n","                #Assign text to a new object:\n","                text_object = {}\n","                text_object = text  \n","\n","                # Process the baseline text:\n","                nlp_doc = nlp(text_object[\"baseline_text\"])\n","\n","                # Create a dictionary from the result:\n","                nlp_dict = {}\n","                nlp_dict = nlp_doc.to_dict()\n","\n","                classla_dict = []\n","\n","                # Add the representations of each word in a dictionary:\n","                for sentence in nlp_dict:\n","                    for word_dict in sentence[0]:\n","                        classla_dict.append(word_dict)\n","\n","                # Store separate representations in strings:\n","                lemma_string = \"\"\n","                upos_string = \"\"\n","                xpos_string = \"\"\n","                ner_string = \"\"\n","                dependency_string = \"\"\n","\n","                # Add the results from the dictionary to strings:\n","                for word in classla_dict:\n","                                lemma_string += word[\"lemma\"]\n","                                lemma_string += \" \"\n","                                upos_string += word['upos']\n","                                upos_string += \" \"\n","                                xpos_string += word['xpos']\n","                                xpos_string += \" \"\n","                                ner_string += word[\"ner\"]\n","                                ner_string += \" \"\n","                                dependency_string += word[\"deprel\"]\n","                                dependency_string += \" \"\n","\n","                # Add the strings to the dataset instances:\n","                text_object[\"lemmas\"] = lemma_string\n","                text_object[\"upos\"] = upos_string\n","                text_object[\"xpos\"] = xpos_string\n","                text_object[\"ner\"] = ner_string\n","                text_object[\"dependency\"] = dependency_string\n","                text_id = text_object[\"id\"]\n","                \n","                # Add the object to the list of objects which will be saved into JSON\n","                json_dict.append(text_object)\n","\n","            # Print out an notification if a document was empty.\n","            else:\n","                print(f\"Document with the id {text_id} is empty. {str(empty_counter)} of documents are empty, not processed.\")\n","        \n","    new_representation_file = open(file_name, \"w\")\n","    json.dump(json_dict,new_representation_file, indent= \"\")\n","    new_representation_file.close()\n","\n","    print(\"Hooray, processing of the dataset is completed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["process_dataset_json(dataset[:3],\"example.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset_rejected = []\n","\n","def multiple_texts_from_long_text(instance):\n","    \"\"\"\n","    Splits a long text into instances with shorter texts (1000 words), appends them\n","    to the dataset with additional parameters: \"short_text\":True and \"position_short_text\"\n","    so that the text can be merged if needed.\n","    Args: instance (dictionary): an object from the the dataset representing one document\n","    \n","    It appends split instances to the dataset_rejected list.\n","    \"\"\"\n","    long_text = deepcopy(instance)\n","    \n","    # Create a list from the baseline text and split it into chunk of 1000 words:\n","    list_long_text = []\n","    list_long_text = long_text[\"baseline_text\"].split()\n","    long_text_chunks = [list_long_text[x:x+1000] for x in range(0, len(list_long_text), 1000)]\n","    \n","    # Merge the words in lists of chunks into strings, create a list of strings:\n","    list_of_long_text_chunks = []\n","\n","    for chunk in long_text_chunks:\n","        split_text = \"\"\n","        split_text = \" \".join(chunk)\n","        new_chunk = []\n","        new_chunk.append(split_text)\n","        list_of_long_text_chunks.append(new_chunk)\n","    \n","    for i in range(len(list_of_long_text_chunks)):\n","        new_text_instance = {}\n","        new_text_instance = deepcopy(long_text)\n","        new_text_instance[\"short_text\"] = True\n","        new_text_instance[\"position_short_text\"] = i+1\n","        new_baseline_text = \"\"\n","        new_baseline_text = list_of_long_text_chunks[i][0]\n","        new_text_instance[\"baseline_text\"] = new_baseline_text\n","        list_baseline_text = []\n","        list_baseline_text = new_baseline_text.split()    \n","        new_text_instance[\"no_of_words\"] = len(list_baseline_text)\n","        dataset_rejected.append(new_text_instance)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["process_dataset_json(dataset_rejected,\"GINCO-rejected-dataset.json\")"]},{"cell_type":"markdown","metadata":{},"source":["Then we would need to merge the instances and add them to the main processed dataset."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
