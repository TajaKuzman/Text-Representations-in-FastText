{"cells":[{"cell_type":"markdown","metadata":{"id":"lQtB43PuAD2H"},"source":["# Preparing the dataset"]},{"cell_type":"markdown","metadata":{},"source":["Importing the necessary libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T11:33:41.284712Z","iopub.status.busy":"2022-02-04T11:33:41.283899Z","iopub.status.idle":"2022-02-04T11:33:41.309102Z","shell.execute_reply":"2022-02-04T11:33:41.308231Z","shell.execute_reply.started":"2022-02-04T11:33:41.284606Z"},"trusted":true},"outputs":[],"source":["import json\n","import pandas as pd\n","from copy import deepcopy\n","import re\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T11:42:27.074930Z","iopub.status.busy":"2022-02-03T11:42:27.074539Z","iopub.status.idle":"2022-02-03T11:42:27.334910Z","shell.execute_reply":"2022-02-03T11:42:27.333513Z","shell.execute_reply.started":"2022-02-03T11:42:27.074889Z"},"executionInfo":{"elapsed":389,"status":"ok","timestamp":1640949108560,"user":{"displayName":"Taja Kuzman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16199926972677634189"},"user_tz":-60},"id":"qL9iYqif_lSk","trusted":true},"outputs":[],"source":["# Import the file\n","\n","with open(\"/kaggle/input/genre-identification-corpus-ginco-10/GINCO-1.0-suitable.json\") as f:\n","    dataset = json.load(f)\n","\n","dataset[0]"]},{"cell_type":"markdown","metadata":{"id":"qn5ISqRR3hpq"},"source":[" ## Extract text from paragraphs into one string: baseline text\n","\n"," We'll create an additional parameter for each document: \"keep_text\" with text non-duplicates and duplicates, marked as \"keep\" (useful for genre identification, although they are duplicates). After joining text, we'll delete the separate paragraphs to make the file smaller."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T11:42:27.336991Z","iopub.status.busy":"2022-02-03T11:42:27.336539Z","iopub.status.idle":"2022-02-03T11:42:27.410617Z","shell.execute_reply":"2022-02-03T11:42:27.409445Z","shell.execute_reply.started":"2022-02-03T11:42:27.336954Z"},"trusted":true},"outputs":[],"source":["for instance in dataset:\n","    paragraphs = instance[\"paragraphs\"]\n","    # Removing duplicates:\n","    paragraphs = [p for p in paragraphs if p[\"keep\"]]\n","\n","    # Joining texts:\n","    instance_keep_text = \" \".join([p[\"text\"] for p in paragraphs])\n","    \n","    # Remove all symbols that raise an error in CLASSLA:\n","    gt_re = re.compile(\"&gt;\")\n","    lt_re = re.compile(\"&lt;\")\n","    instance_keep_text = gt_re.sub('',instance_keep_text)\n","    instance_keep_text = lt_re.sub('',instance_keep_text)\n","    \n","    # Assigning texts to a new field:\n","    instance[\"baseline_text\"] = instance_keep_text\n","    \n","    # Delete the text in paragraphs to save space:\n","    instance[\"paragraphs\"] = []\n","    \n","    # Add a new field for text length:\n","    list_baseline_text = []\n","    list_baseline_text = instance[\"baseline_text\"].split()    \n","    instance[\"no_of_words\"] = len(list_baseline_text)\n","    \n","dataset[0]"]},{"cell_type":"markdown","metadata":{},"source":["## Add text representations"]},{"cell_type":"markdown","metadata":{},"source":["Import the CLASSLA Pipeline for processing of Slovene"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T11:42:27.413378Z","iopub.status.busy":"2022-02-03T11:42:27.413060Z","iopub.status.idle":"2022-02-03T11:42:38.700004Z","shell.execute_reply":"2022-02-03T11:42:38.698724Z","shell.execute_reply.started":"2022-02-03T11:42:27.413348Z"},"trusted":true},"outputs":[],"source":["!pip install classla"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T11:42:38.702174Z","iopub.status.busy":"2022-02-03T11:42:38.701878Z"},"trusted":true},"outputs":[],"source":["# Import CLASSLA for language processing\n","import classla\n","classla.download(\"sl\")\n","nlp = classla.Pipeline('sl')"]},{"cell_type":"markdown","metadata":{},"source":["Let's first see how the pipeline works by processing the first text:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Run the processing on the first text\n","doc = nlp(dataset[0][\"baseline_text\"])\n","\n","dictionary = doc.to_dict()\n","dictionary[0][0][:2]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def process_dataset_json(dataset,file_name):\n","    \"\"\"\n","    This function performs linguistic processing of the text in the text[\"baseline_text\"] value\n","    by using the CLASSLA pipeline for processing of South Slavic languages.\n","    It adds keys lemmas, upos, xpos, ner and dependency to each dictionary element\n","    in the dataset containing information on each document.\n","    Finally, it adds the update content for each text dictionary\n","    to a JSON file which name is specified with the argument file_name.\n","    \n","    Be aware that if a text contains more than 3000 words, you might need to split the text\n","    into multiple instances to avoid overloading the memory.\n","\n","    Args:\n","        dataset (str): name of the dataset object\n","        file_name(str): name of the JSON file where the new dataset is written\n","    \"\"\"\n","    json_dict = []\n","        \n","    for text in tqdm(dataset):\n","            if len(text[\"baseline_text\"]) > 0:\n","                #Assign text to a new object:\n","                text_object = {}\n","                text_object = text  \n","\n","                # Process the baseline text:\n","                nlp_doc = nlp(text_object[\"baseline_text\"])\n","\n","                # Create a dictionary from the result:\n","                nlp_dict = {}\n","                nlp_dict = nlp_doc.to_dict()\n","\n","                classla_dict = []\n","\n","                # Add the representations of each word in a dictionary:\n","                for sentence in nlp_dict:\n","                    for word_dict in sentence[0]:\n","                        classla_dict.append(word_dict)\n","\n","                # Store separate representations in strings:\n","                lemma_string = \"\"\n","                upos_string = \"\"\n","                xpos_string = \"\"\n","                ner_string = \"\"\n","                dependency_string = \"\"\n","\n","                # Add the results from the dictionary to strings:\n","                for word in classla_dict:\n","                                lemma_string += word[\"lemma\"]\n","                                lemma_string += \" \"\n","                                upos_string += word['upos']\n","                                upos_string += \" \"\n","                                xpos_string += word['xpos']\n","                                xpos_string += \" \"\n","                                ner_string += word[\"ner\"]\n","                                ner_string += \" \"\n","                                dependency_string += word[\"deprel\"]\n","                                dependency_string += \" \"\n","\n","                # Add the strings to the dataset instances:\n","                text_object[\"lemmas\"] = lemma_string\n","                text_object[\"upos\"] = upos_string\n","                text_object[\"xpos\"] = xpos_string\n","                text_object[\"ner\"] = ner_string\n","                text_object[\"dependency\"] = dependency_string\n","                text_id = text_object[\"id\"]\n","                \n","                # Add the object to the list of objects which will be saved into JSON\n","                json_dict.append(text_object)\n","\n","            # Print out an notification if a document was empty.\n","            else:\n","                print(f\"Document with the id {text_id} is empty. {str(empty_counter)} of documents are empty, not processed.\")\n","        \n","    new_representation_file = open(file_name, \"w\")\n","    json.dump(json_dict,new_representation_file, indent= \"\")\n","    new_representation_file.close()\n","\n","    print(\"Hooray, processing of the dataset is completed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["process_dataset_json(dataset[:3],\"example.json\")"]},{"cell_type":"markdown","metadata":{},"source":["If the text is longer than 3.000 words, we might run into an error if the memory is overloaded. In this case, we can split the text into multiple texts and process them separately."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset_rejected = []\n","\n","def multiple_texts_from_long_text(instance):\n","    \"\"\"\n","    Splits a long text into instances with shorter texts (1000 words), appends them\n","    to the dataset with additional parameters: \"short_text\":True and \"position_short_text\"\n","    so that the text can be merged if needed.\n","    Args: instance (dictionary): an object from the the dataset representing one document\n","    \n","    It appends split instances to the dataset_rejected list.\n","    \"\"\"\n","    long_text = deepcopy(instance)\n","    \n","    # Create a list from the baseline text and split it into chunk of 1000 words:\n","    list_long_text = []\n","    list_long_text = long_text[\"baseline_text\"].split()\n","    long_text_chunks = [list_long_text[x:x+1000] for x in range(0, len(list_long_text), 1000)]\n","    \n","    # Merge the words in lists of chunks into strings, create a list of strings:\n","    list_of_long_text_chunks = []\n","\n","    for chunk in long_text_chunks:\n","        split_text = \"\"\n","        split_text = \" \".join(chunk)\n","        new_chunk = []\n","        new_chunk.append(split_text)\n","        list_of_long_text_chunks.append(new_chunk)\n","    \n","    for i in range(len(list_of_long_text_chunks)):\n","        new_text_instance = {}\n","        new_text_instance = deepcopy(long_text)\n","        new_text_instance[\"short_text\"] = True\n","        new_text_instance[\"position_short_text\"] = i+1\n","        new_baseline_text = \"\"\n","        new_baseline_text = list_of_long_text_chunks[i][0]\n","        new_text_instance[\"baseline_text\"] = new_baseline_text\n","        list_baseline_text = []\n","        list_baseline_text = new_baseline_text.split()    \n","        new_text_instance[\"no_of_words\"] = len(list_baseline_text)\n","        dataset_rejected.append(new_text_instance)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["process_dataset_json(dataset_rejected,\"GINCO-rejected-dataset.json\")"]},{"cell_type":"markdown","metadata":{},"source":["Then we would need to merge the instances and add them to the main processed dataset."]},{"cell_type":"markdown","metadata":{},"source":["## Adding additional representations"]},{"cell_type":"markdown","metadata":{},"source":["Let's import the created GINCO file with representations created with the CLASSLA pipeline, and add some more representations: text in lowercase, punctuation removed, and a list of representations which allows us further transformations (using just nouns etc.)."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T11:33:54.303572Z","iopub.status.busy":"2022-02-04T11:33:54.302910Z","iopub.status.idle":"2022-02-04T11:33:54.666689Z","shell.execute_reply":"2022-02-04T11:33:54.665739Z","shell.execute_reply.started":"2022-02-04T11:33:54.303442Z"},"trusted":true},"outputs":[],"source":["with open(\"/kaggle/input/ginco-with-additional-text-representations/GINCO-dataset-with-additional-textual-representations.json\") as file:\n","    dataset = json.load(file)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T11:33:56.044876Z","iopub.status.busy":"2022-02-04T11:33:56.044560Z","iopub.status.idle":"2022-02-04T11:33:56.055759Z","shell.execute_reply":"2022-02-04T11:33:56.054549Z","shell.execute_reply.started":"2022-02-04T11:33:56.044842Z"},"trusted":true},"outputs":[],"source":["dataset[0]"]},{"cell_type":"markdown","metadata":{},"source":["Add lower cased text:"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T11:34:16.317134Z","iopub.status.busy":"2022-02-04T11:34:16.316862Z","iopub.status.idle":"2022-02-04T11:34:16.325838Z","shell.execute_reply":"2022-02-04T11:34:16.324831Z","shell.execute_reply.started":"2022-02-04T11:34:16.317104Z"},"trusted":true},"outputs":[],"source":["def lowercase_nopunctuation(dataset):\n","    \"\"\"\n","    This function adds to each instance of the dataset keys \"lowercase\" and \"nopunctuation\"\n","    where the baseline text is transformed into lower case, and then the punctuation is removed.\n","    \n","    Args:\n","        dataset (object) = name of the dataset\n","    \"\"\"\n","    from string import punctuation\n","    \n","    for instance in tqdm(dataset):\n","        text = instance[\"baseline_text\"]\n","        \n","        # split text into tokens by white space\n","        token = text.split()\n","        \n","        # recalculate the number of words\n","        instance[\"no_of_words\"] = len(token)\n","        \n","        # make all words lowercase\n","        token = [word.lower() for word in token]\n","        \n","        # add a new key with lowercase words to the instance\n","        instance[\"lowercase\"] = \" \".join(token)\n","        \n","        # remove punctuation from each token\n","        table = str.maketrans('', '', punctuation)\n","        token = [word.translate(table) for word in token]\n","\n","        # add a new key with lowercase words + punctuation removed\n","        instance[\"lowercase_nopunctuation\"] = \" \".join(token)\n","        \n","    print(\"Hooray, the dataset is processed.\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T11:34:26.175379Z","iopub.status.busy":"2022-02-04T11:34:26.174899Z","iopub.status.idle":"2022-02-04T11:34:26.543902Z","shell.execute_reply":"2022-02-04T11:34:26.542943Z","shell.execute_reply.started":"2022-02-04T11:34:26.175329Z"},"trusted":true},"outputs":[],"source":["lowercase_nopunctuation(dataset)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T11:34:30.357414Z","iopub.status.busy":"2022-02-04T11:34:30.357118Z","iopub.status.idle":"2022-02-04T11:34:30.366546Z","shell.execute_reply":"2022-02-04T11:34:30.365774Z","shell.execute_reply.started":"2022-02-04T11:34:30.357381Z"},"trusted":true},"outputs":[],"source":["dataset[:3]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T11:35:16.061819Z","iopub.status.busy":"2022-02-04T11:35:16.061506Z","iopub.status.idle":"2022-02-04T11:35:16.073098Z","shell.execute_reply":"2022-02-04T11:35:16.072049Z","shell.execute_reply.started":"2022-02-04T11:35:16.061761Z"},"trusted":true},"outputs":[],"source":["def list_of_representations(dataset):\n","    \"\"\"\n","    This function creates a list of lists of different representations\n","    where each inner list contains a lemma and all its representations.\n","    The list is appended to each element of the dataset under the key \"representation_list\".\n","    \n","    Args:\n","        dataset(object) = name of the dataset\n","    \"\"\"\n","\n","    for instance in tqdm(dataset):\n","        lemmas = instance['lemmas'].split()\n","        upos = instance['upos'].split()\n","        xpos = instance ['xpos'].split()\n","        ner = instance['ner'].split()\n","        dependency = instance['dependency'].split()\n","        \n","        if len(lemmas) == len(upos) == len(xpos) == len(ner) == len(dependency):\n","\n","            representations_list = []\n","\n","            for i in [lemmas, upos, xpos, ner, dependency]:\n","                representations_list.append(i)\n","\n","            representation_sets = []\n","\n","            # variable to 0\n","            index = 0\n","\n","            for token in range(len(representations_list[0])):\n","            # appending an empty sub_list\n","                representation_sets.append([])\n","            # iterating over the list length\n","                for representation in range(len(representations_list)):\n","                  # adding the element to the result\n","                    representation_sets[index].append(representations_list[representation][index])\n","\n","                # moving to the next index\n","                index += 1\n","            \n","            instance[\"representation_list\"] = representation_sets\n","            \n","        else:\n","            text_id = instance[\"id\"]\n","            print(f\"The lengths for text with id {text_id} don't match!!\")\n","    \n","    print(\"Hooray, the representation lists are created for all texts.\")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T11:35:22.883982Z","iopub.status.busy":"2022-02-04T11:35:22.883113Z","iopub.status.idle":"2022-02-04T11:35:24.275995Z","shell.execute_reply":"2022-02-04T11:35:24.275098Z","shell.execute_reply.started":"2022-02-04T11:35:22.883941Z"},"trusted":true},"outputs":[],"source":["list_of_representations(dataset)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T11:35:34.078516Z","iopub.status.busy":"2022-02-04T11:35:34.078211Z","iopub.status.idle":"2022-02-04T11:35:34.215682Z","shell.execute_reply":"2022-02-04T11:35:34.214808Z","shell.execute_reply.started":"2022-02-04T11:35:34.078481Z"},"trusted":true},"outputs":[],"source":["dataset[-3:]"]},{"cell_type":"markdown","metadata":{},"source":["Let's save the new file:"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T11:39:25.880473Z","iopub.status.busy":"2022-02-04T11:39:25.880186Z","iopub.status.idle":"2022-02-04T11:39:29.144996Z","shell.execute_reply":"2022-02-04T11:39:29.144121Z","shell.execute_reply.started":"2022-02-04T11:39:25.880442Z"},"trusted":true},"outputs":[],"source":["new_GINCO_file = open(\"Language-Processed-GINCO.json\", \"w\")\n","json.dump(dataset,new_GINCO_file, indent= \"\")\n","new_GINCO_file.close()"]},{"cell_type":"markdown","metadata":{},"source":["This is an example of how we can add a new representation where words of certain type are merged into one token (in the case below \"1\"), or we could also just remove them."]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T11:14:08.981169Z","iopub.status.busy":"2022-02-04T11:14:08.980874Z","iopub.status.idle":"2022-02-04T11:14:08.997588Z","shell.execute_reply":"2022-02-04T11:14:08.996857Z","shell.execute_reply.started":"2022-02-04T11:14:08.981135Z"},"trusted":true},"outputs":[],"source":["instance = dataset[0]\n","\n","list_no_numbers = []\n","\n","for token in instance[\"representation_list\"]:\n","    if token[1] != 'NUM':\n","        list_no_numbers.append(token[0])\n","    else:\n","        list_no_numbers.append(\"1\")\n","\n","instance[\"numbers_reduction\"] = \" \".join(list_no_numbers)\n","\n","instance"]},{"cell_type":"markdown","metadata":{},"source":["Use this code if you wish to create a new representation containing just lemmas of a specific word type:"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T11:25:40.858557Z","iopub.status.busy":"2022-02-04T11:25:40.858042Z","iopub.status.idle":"2022-02-04T11:25:40.865221Z","shell.execute_reply":"2022-02-04T11:25:40.864228Z","shell.execute_reply.started":"2022-02-04T11:25:40.858520Z"},"trusted":true},"outputs":[],"source":["def only_specific_word_types(dataset, key_name, representation_place, tag):\n","    \"\"\"\n","    This function creates an additional representation (of lemmas) containing just the desired word type\n","    based on the 'representation_list' which contains lemmas and various linguistic tags.\n","    \n","    Args:\n","        dataset(object) = name of the dataset\n","        key_name (string) = name of the new key with the created representation\n","        representation_place (int) = specify in which representation you're interested in:\n","            upos - 1, xpos - 2, ner - 3, dependency - 4\n","        tag (str) = specify in which tag you're interested in (e.g. NOUN, PUNCT, ADJ ...)\n","    \"\"\"\n","    \n","    for instance in tqdm(dataset):\n","        list_new_representation = []\n","\n","        for token in instance[\"representation_list\"]:\n","            if token[representation_place] == tag:\n","                list_new_representation.append(token[0])\n","\n","        instance[key_name] = \" \".join(list_new_representation)\n","    \n","    print(\"New representation added :)\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"4054637ba3b142bc5aa565ef637e29d85952330aa10ad5bd25955ba00ae91bfa"}}},"nbformat":4,"nbformat_minor":4}
