{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Learning on various text representations","metadata":{}},{"cell_type":"markdown","source":"## Preparing the dataset for FastText","metadata":{}},{"cell_type":"markdown","source":"Importing the necessary libraries","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom copy import deepcopy\nimport re\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:14:19.885492Z","iopub.execute_input":"2022-02-11T10:14:19.885851Z","iopub.status.idle":"2022-02-11T10:14:19.915615Z","shell.execute_reply.started":"2022-02-11T10:14:19.885763Z","shell.execute_reply":"2022-02-11T10:14:19.914828Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Import the file with additional text representations (only the paragraphs marked to be kept\n# in the original corpus are included)\n\nwith open(\"/kaggle/input/ginco-with-additional-text-representations/Language-Processed-GINCO.json\") as f:\n    dataset = json.load(f)\n\ndataset[0]","metadata":{"executionInfo":{"elapsed":389,"status":"ok","timestamp":1640949108560,"user":{"displayName":"Taja Kuzman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16199926972677634189"},"user_tz":-60},"id":"qL9iYqif_lSk","execution":{"iopub.status.busy":"2022-02-11T10:14:25.954116Z","iopub.execute_input":"2022-02-11T10:14:25.954732Z","iopub.status.idle":"2022-02-11T10:14:27.429913Z","shell.execute_reply.started":"2022-02-11T10:14:25.954693Z","shell.execute_reply":"2022-02-11T10:14:27.428735Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"dataset[0].keys()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:15:38.007401Z","iopub.execute_input":"2022-02-11T10:15:38.007705Z","iopub.status.idle":"2022-02-11T10:15:38.013690Z","shell.execute_reply.started":"2022-02-11T10:15:38.007665Z","shell.execute_reply":"2022-02-11T10:15:38.012934Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Pre-processing dataset","metadata":{}},{"cell_type":"markdown","source":"Here we can create additional representations if we wish (see the notebook *2-Language-Processing-of-GINCO*).","metadata":{}},{"cell_type":"markdown","source":"1. Remove punctuation from each token","metadata":{}},{"cell_type":"code","source":"        for instance in tqdm(dataset):\n        text = instance[\"baseline_text\"]\n        \n        # split text into tokens by white space\n        token = text.split()\n        \n         \n        # remove punctuation from each token\n        table = str.maketrans('', '', punctuation)\n        token = [word.translate(table) for word in token]\n\n        # add a new key with punctuation removed\n        instance[\"nopunctuation\"] = \" \".join(token)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Remove numbers from each token","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. Remove stopwords","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Apply all (lowercase, remove punctuation, numbers, stopwords)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Downcasting number of labels","metadata":{}},{"cell_type":"markdown","source":"In these experiments, we will not use all of the texts but only texts from 5 main categories, meaning that some categories will be merged into them, whereas some categories with a very small frequency will be discarded. Additionally, the texts marked us hard, will be discarded (see notebook *1-Preparing_Data_Hyperparameter_Search*).\n\nWe will start with a reduced set of labels (primary_level_3), then merge News and Opinionated News, and discard some of the lables.","metadata":{}},{"cell_type":"code","source":"# merge News and Opinionated News\nfor i in dataset:\n    if i[\"primary_level_3\"] == \"Opinionated News\" or i[\"primary_level_3\"] == \"News/Reporting\":\n        i[\"primary_level_3\"] = \"News\"","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:15:57.238821Z","iopub.execute_input":"2022-02-11T10:15:57.239235Z","iopub.status.idle":"2022-02-11T10:15:57.246029Z","shell.execute_reply.started":"2022-02-11T10:15:57.239201Z","shell.execute_reply":"2022-02-11T10:15:57.245063Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Let's create train:test:dev split that contains only the wanted labels.","metadata":{}},{"cell_type":"code","source":"downcasted_labels = ['Information/Explanation', 'Promotion', 'News', 'Forum', 'Opinion/Argumentation']\n\ntrain = [i for i in dataset if i[\"split\"] == \"train\" and i[\"primary_level_3\"] in downcasted_labels and not i[\"hard\"]]\ntest = [i for i in dataset if i[\"split\"] == \"test\" and i[\"primary_level_3\"] in downcasted_labels and not i[\"hard\"]]\ndev = [i for i in dataset if i[\"split\"] == \"dev\" and i[\"primary_level_3\"] in downcasted_labels and not i[\"hard\"]]\n\nprint(\"The train-dev-test splits consist of the following numbers of examples:\", len(train), len(test), len(dev))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:16:02.431850Z","iopub.execute_input":"2022-02-11T10:16:02.432211Z","iopub.status.idle":"2022-02-11T10:16:02.444716Z","shell.execute_reply.started":"2022-02-11T10:16:02.432175Z","shell.execute_reply":"2022-02-11T10:16:02.443592Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(f\"Number of all texts is {len(train)+len(test)+len(dev)}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:16:10.633889Z","iopub.execute_input":"2022-02-11T10:16:10.634349Z","iopub.status.idle":"2022-02-11T10:16:10.639450Z","shell.execute_reply.started":"2022-02-11T10:16:10.634311Z","shell.execute_reply":"2022-02-11T10:16:10.638608Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Creating FastText texts","metadata":{}},{"cell_type":"markdown","source":"Now, let's create FastText train and test files, containing the desired representation.","metadata":{"id":"xw1ugH8i6KuD"}},{"cell_type":"code","source":"def fastText_files(representation):\n    \"\"\"\n    This function creates and saves the test and train file\n    from the test, train and dev split of the dataset (named test, dev and train),\n    using the \"primary_level_3\" level labels, and the chosen text representation.\n    \n    Possible representations: 'baseline_text', 'lemmas',\n    'upos', 'xpos', 'ner', 'dependency', 'lowercase', 'lowercase_nopunctuation'\n    \n    The function returns a list of the following elements:\n        - labels - which can be used for prediction and evaluation.\n        - train file path\n        - test file path\n    \n    Args:\n        representation (str): the name of the key (from the dataset)\n                                of the text representation we want to use\n    \"\"\"\n    # First create the dataframes from each split:\n    \n    train_df = pd.DataFrame(data=train, columns=[representation, \"primary_level_3\"])\n    # Renaming columns to `text` and `labels`\n    train_df.columns = [\"text\", \"labels\"]\n    \n    test_df = pd.DataFrame(data=test, columns=[representation, \"primary_level_3\"])\n    test_df.columns = [\"text\", \"labels\"]\n    \n    print(\"The shape of the dataframes:\")\n    print(train_df.shape, test_df.shape)\n    \n    # Then create CSV files which FastText can read\n    \n    train_file_content=\"\"\n\n    for labels, text in train_df.loc[:, [\"labels\", \"text\"]].values:\n        label = f\"__label__{labels}\"\n        train_file_content += f\"\"\"{label} {text}\\n\"\"\"\n    \n    train_path = \"\"\n    train_path = representation + \"-fasttext.train\"\n\n    with open(train_path,\"w\") as train_file:\n        train_file.write(train_file_content)\n    \n    train_example = open(train_path,\"r\").read(1000)\n    print(\"Created train file:\")\n    print(train_example)\n    \n    test_file_content=\"\"\n    \n    for labels, text in test_df.loc[:, [\"labels\", \"text\"]].values:\n        label = f\"__label__{labels}\"\n        test_file_content += f\"\"\"{label} {text}\\n\"\"\"\n    \n    test_path = \"\"\n    test_path = representation + \"-fasttext.test\"\n    \n    with open(test_path,\"w\") as test_file:\n        test_file.write(test_file_content)\n    \n    test_example = open(test_path,\"r\").read(1000)\n    print(\"Created test file:\")\n    print(test_example)\n    \n    \n    # Finally, create a list of labels which can be used for prediction and evaluation.\n    # Let's inspect the labels:\n    all_df_labels = train_df[\"labels\"].unique().tolist()\n    \n    for i in test_df[\"labels\"].unique().tolist():\n        if i not in all_df_labels:\n            all_df_labels.append(i)\n\n    print(f\"Number of all labels: {len(all_df_labels)}\")\n    \n    # Create a final list of labels in a FastText-appropriate format:\n    LABELS = train_df.labels.unique().tolist()\n    LABELS = [f\"__label__{i}\" for i in LABELS]\n    \n    return_list = [LABELS, train_path, test_path]\n    print(f\"The function returned the following list: {return_list}\")\n    \n    return return_list","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:25:25.187324Z","iopub.execute_input":"2022-02-11T10:25:25.187653Z","iopub.status.idle":"2022-02-11T10:25:25.204382Z","shell.execute_reply.started":"2022-02-11T10:25:25.187619Z","shell.execute_reply":"2022-02-11T10:25:25.203237Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"lowercase_representation = fastText_files('lowercase')","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:25:28.144504Z","iopub.execute_input":"2022-02-11T10:25:28.144790Z","iopub.status.idle":"2022-02-11T10:25:28.168328Z","shell.execute_reply.started":"2022-02-11T10:25:28.144759Z","shell.execute_reply":"2022-02-11T10:25:28.167703Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Define the label list:\nLABELS = lowercase_representation[0]\n\nLABELS","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:21:40.848811Z","iopub.execute_input":"2022-02-11T10:21:40.849226Z","iopub.status.idle":"2022-02-11T10:21:40.856571Z","shell.execute_reply.started":"2022-02-11T10:21:40.849182Z","shell.execute_reply":"2022-02-11T10:21:40.855778Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Train a fastText model","metadata":{"id":"QB0_-1xIRPx-"}},{"cell_type":"markdown","source":"Importing the necessary libraries","metadata":{}},{"cell_type":"code","source":"!pip install parse","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:26:03.564099Z","iopub.execute_input":"2022-02-11T10:26:03.564497Z","iopub.status.idle":"2022-02-11T10:26:18.798073Z","shell.execute_reply.started":"2022-02-11T10:26:03.564458Z","shell.execute_reply":"2022-02-11T10:26:18.796873Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import fasttext as ft\nimport parse\nimport numpy as np\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:26:18.801075Z","iopub.execute_input":"2022-02-11T10:26:18.801444Z","iopub.status.idle":"2022-02-11T10:26:20.047908Z","shell.execute_reply.started":"2022-02-11T10:26:18.801407Z","shell.execute_reply":"2022-02-11T10:26:20.047146Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Input the data:","metadata":{}},{"cell_type":"code","source":"FT_train_file = lowercase_representation[1]\nFT_test_file = lowercase_representation[2]","metadata":{"id":"s3ytA4GWVVhk","execution":{"iopub.status.busy":"2022-02-11T10:32:09.203048Z","iopub.execute_input":"2022-02-11T10:32:09.203411Z","iopub.status.idle":"2022-02-11T10:32:09.208994Z","shell.execute_reply.started":"2022-02-11T10:32:09.203370Z","shell.execute_reply":"2022-02-11T10:32:09.207728Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{"id":"Ewnh-3ZlmOF-"}},{"cell_type":"code","source":"def parse_test_file(path: str):\n    \"\"\"Reads fasttext formatted file and returns labels, texts.\"\"\"\n    with open(path, \"r\") as f:\n        content = f.readlines()\n    pattern = \"{label} {text}\\n\"\n    p = parse.compile(pattern)\n\n    labels, texts = list(), list()\n    for line in content:\n        rez = p.parse(line)\n        if rez is not None:\n            labels.append(rez[\"label\"])\n            texts.append(rez[\"text\"])\n        else:\n            print(\"error parsing line \", line)\n    return labels, texts\n\ndef prediction_to_label(prediction):\n    \"\"\"Transforms predictions as returned by fasttext into pure labels.\"\"\"\n    return np.array(prediction[0])[:, 0]\n\ndef plot_cm(save=False, title=None):\n    \"\"\"\n    Plots confusion matrix for prediction on the test set.\n    Takes the predictions, named as y_pred, true values, named as y_true,\n    and labels, named as LABELS.\n    \n    Arguments:\n        save: whether the confusion matrix is saved. Defaults to False.\n        title: the title of the confusion matrix. Defaults to None.\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred, labels=LABELS)\n    plt.figure(figsize=(9, 9))\n    plt.imshow(cm, cmap=\"Oranges\")\n    classNames = LABELS\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames, rotation=90)\n    plt.yticks(tick_marks, classNames)\n    microF1 = f1_score(y_true, y_pred, labels=LABELS, average =\"micro\")\n    macroF1 = f1_score(y_true, y_pred, labels=LABELS, average =\"macro\")\n\n    print(f\"{microF1:0.4}\")\n    print(f\"{macroF1:0.4}\")\n\n    metrics = f\"{microF1:0.4}, {macroF1:0.4}\"\n    if title:\n        plt.title(title +\";\\n\" + metrics)\n    else:\n        plt.title(metrics)\n    plt.tight_layout()\n    if save:\n        plt.savefig(save)\n    plt.show()\n    return microF1, macroF1","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:32:12.278923Z","iopub.execute_input":"2022-02-11T10:32:12.279875Z","iopub.status.idle":"2022-02-11T10:32:12.294857Z","shell.execute_reply.started":"2022-02-11T10:32:12.279832Z","shell.execute_reply":"2022-02-11T10:32:12.294011Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"results = list()\n\nfor i in range(5):\n    model = ft.train_supervised(input=FT_train_file,\n                                epoch = 350,\n                                lr = 0.7,\n                                wordNgrams=1,\n                                verbose = 2\n                                            )\n    # Parse the test files so that labels and texts are separated\n    y_true, y_texts = parse_test_file(FT_test_file)\n\n    # Evaluate te model on test data\n    y_pred = model.predict(y_texts)\n    y_pred = prediction_to_label(y_pred)\n\n    # Plot the confusion matrix:\n    m, M = plot_cm(save=False, title=f\"Run: {i}\")\n    \n    rezdict = dict(\n        microF1=m,\n        macroF1=M,\n        run=i\n    )\n    results.append(rezdict)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:32:37.575253Z","iopub.execute_input":"2022-02-11T10:32:37.575516Z","iopub.status.idle":"2022-02-11T10:33:09.488336Z","shell.execute_reply.started":"2022-02-11T10:32:37.575486Z","shell.execute_reply":"2022-02-11T10:33:09.487160Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.execute_input":"2022-02-04T12:58:41.628207Z","iopub.status.busy":"2022-02-04T12:58:41.627915Z","iopub.status.idle":"2022-02-04T12:58:41.634578Z","shell.execute_reply":"2022-02-04T12:58:41.633796Z","shell.execute_reply.started":"2022-02-04T12:58:41.628179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n\nplt.style.use('_mpl-gallery')\n\n# make data\n\nx = []\nmi = []\nma = []\n\nfor i in results:\n    x.append(i['run'])\n    mi.append(i['microF1'])\n    ma.append(i[\"macroF1\"])\n\n# plot\nfig, ax = plt.subplots(figsize=(6,3), dpi=100)\n\nax.plot(x, mi, linewidth=2.0, label=\"Micro F1\")\nax.plot(x, ma, linewidth=2.0, label=\"Macro F1\")\n\nax.set(xlim=(0,4.3),ylim=(0.5, 0.7),xticks=[0,1,2,3,4])\n\nax.set_xlabel('Runs')\nax.set_ylabel('F1 Score')\nax.legend(loc=\"lower right\")\n\nplt.tight_layout()\n#plt.savefig(\"N-grams.png\")\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2022-02-04T12:58:41.636272Z","iopub.status.busy":"2022-02-04T12:58:41.635642Z","iopub.status.idle":"2022-02-04T12:58:41.874439Z","shell.execute_reply":"2022-02-04T12:58:41.873590Z","shell.execute_reply.started":"2022-02-04T12:58:41.636237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"micro F1: {np.array(mi).mean():0.03} +/- {np.array(mi).std():0.02}\")\nprint(f\"macro F1: {np.array(ma).mean():0.03} +/- {np.array(ma).std():0.02}\")","metadata":{"execution":{"iopub.execute_input":"2022-02-04T12:58:41.875916Z","iopub.status.busy":"2022-02-04T12:58:41.875676Z","iopub.status.idle":"2022-02-04T12:58:41.881547Z","shell.execute_reply":"2022-02-04T12:58:41.880933Z","shell.execute_reply.started":"2022-02-04T12:58:41.875886Z"},"trusted":true},"execution_count":null,"outputs":[]}]}